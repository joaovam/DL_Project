{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration GPU/CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Split dataset into train and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Original VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [100/2500], Loss: 2.3900\n",
      "Epoch [1/30], Step [200/2500], Loss: 2.1761\n",
      "Epoch [1/30], Step [300/2500], Loss: 2.1134\n",
      "Epoch [1/30], Step [400/2500], Loss: 2.0635\n",
      "Epoch [1/30], Step [500/2500], Loss: 2.0372\n",
      "Epoch [1/30], Step [600/2500], Loss: 1.9912\n",
      "Epoch [1/30], Step [700/2500], Loss: 1.9917\n",
      "Epoch [1/30], Step [800/2500], Loss: 1.9270\n",
      "Epoch [1/30], Step [900/2500], Loss: 1.9246\n",
      "Epoch [1/30], Step [1000/2500], Loss: 1.9010\n",
      "Epoch [1/30], Step [1100/2500], Loss: 1.8907\n",
      "Epoch [1/30], Step [1200/2500], Loss: 1.8640\n",
      "Epoch [1/30], Step [1300/2500], Loss: 1.8447\n",
      "Epoch [1/30], Step [1400/2500], Loss: 1.9030\n",
      "Epoch [1/30], Step [1500/2500], Loss: 1.8413\n",
      "Epoch [1/30], Step [1600/2500], Loss: 1.8235\n",
      "Epoch [1/30], Step [1700/2500], Loss: 1.8019\n",
      "Epoch [1/30], Step [1800/2500], Loss: 1.7990\n",
      "Epoch [1/30], Step [1900/2500], Loss: 1.7392\n",
      "Epoch [1/30], Step [2000/2500], Loss: 1.7513\n",
      "Epoch [1/30], Step [2100/2500], Loss: 1.7388\n",
      "Epoch [1/30], Step [2200/2500], Loss: 1.7123\n",
      "Epoch [1/30], Step [2300/2500], Loss: 1.7153\n",
      "Epoch [1/30], Step [2400/2500], Loss: 1.6786\n",
      "Epoch [1/30], Step [2500/2500], Loss: 1.6889\n",
      "Epoch [1/30], Validation Loss: 1.6599, Validation Accuracy: 38.57%\n",
      "Epoch [2/30], Step [100/2500], Loss: 1.9501\n",
      "Epoch [2/30], Step [200/2500], Loss: 1.9076\n",
      "Epoch [2/30], Step [300/2500], Loss: 1.8926\n",
      "Epoch [2/30], Step [400/2500], Loss: 1.8346\n",
      "Epoch [2/30], Step [500/2500], Loss: 1.8068\n",
      "Epoch [2/30], Step [600/2500], Loss: 1.7399\n",
      "Epoch [2/30], Step [700/2500], Loss: 1.7638\n",
      "Epoch [2/30], Step [800/2500], Loss: 1.6945\n",
      "Epoch [2/30], Step [900/2500], Loss: 1.7175\n",
      "Epoch [2/30], Step [1000/2500], Loss: 1.7027\n",
      "Epoch [2/30], Step [1100/2500], Loss: 1.6827\n",
      "Epoch [2/30], Step [1200/2500], Loss: 1.6609\n",
      "Epoch [2/30], Step [1300/2500], Loss: 1.6667\n",
      "Epoch [2/30], Step [1400/2500], Loss: 1.6134\n",
      "Epoch [2/30], Step [1500/2500], Loss: 1.6626\n",
      "Epoch [2/30], Step [1600/2500], Loss: 1.6133\n",
      "Epoch [2/30], Step [1700/2500], Loss: 1.6246\n",
      "Epoch [2/30], Step [1800/2500], Loss: 1.6051\n",
      "Epoch [2/30], Step [1900/2500], Loss: 1.5890\n",
      "Epoch [2/30], Step [2000/2500], Loss: 1.5812\n",
      "Epoch [2/30], Step [2100/2500], Loss: 1.5520\n",
      "Epoch [2/30], Step [2200/2500], Loss: 1.5445\n",
      "Epoch [2/30], Step [2300/2500], Loss: 1.5478\n",
      "Epoch [2/30], Step [2400/2500], Loss: 1.5415\n",
      "Epoch [2/30], Step [2500/2500], Loss: 1.5169\n",
      "Epoch [2/30], Validation Loss: 1.6216, Validation Accuracy: 39.14%\n",
      "Epoch [3/30], Step [100/2500], Loss: 1.5626\n",
      "Epoch [3/30], Step [200/2500], Loss: 1.5809\n",
      "Epoch [3/30], Step [300/2500], Loss: 1.5596\n",
      "Epoch [3/30], Step [400/2500], Loss: 1.5100\n",
      "Epoch [3/30], Step [500/2500], Loss: 1.5720\n",
      "Epoch [3/30], Step [600/2500], Loss: 1.5143\n",
      "Epoch [3/30], Step [700/2500], Loss: 1.5099\n",
      "Epoch [3/30], Step [800/2500], Loss: 1.5449\n",
      "Epoch [3/30], Step [900/2500], Loss: 1.5147\n",
      "Epoch [3/30], Step [1000/2500], Loss: 1.4897\n",
      "Epoch [3/30], Step [1100/2500], Loss: 1.4982\n",
      "Epoch [3/30], Step [1200/2500], Loss: 1.5166\n",
      "Epoch [3/30], Step [1300/2500], Loss: 1.5675\n",
      "Epoch [3/30], Step [1400/2500], Loss: 1.5209\n",
      "Epoch [3/30], Step [1500/2500], Loss: 1.4616\n",
      "Epoch [3/30], Step [1600/2500], Loss: 1.4146\n",
      "Epoch [3/30], Step [1700/2500], Loss: 1.5092\n",
      "Epoch [3/30], Step [1800/2500], Loss: 1.5231\n",
      "Epoch [3/30], Step [1900/2500], Loss: 1.5341\n",
      "Epoch [3/30], Step [2000/2500], Loss: 1.5495\n",
      "Epoch [3/30], Step [2100/2500], Loss: 1.4375\n",
      "Epoch [3/30], Step [2200/2500], Loss: 1.4987\n",
      "Epoch [3/30], Step [2300/2500], Loss: 1.4608\n",
      "Epoch [3/30], Step [2400/2500], Loss: 1.4670\n",
      "Epoch [3/30], Step [2500/2500], Loss: 1.4389\n",
      "Epoch [3/30], Validation Loss: 1.5296, Validation Accuracy: 39.93%\n",
      "Epoch [4/30], Step [100/2500], Loss: 1.5039\n",
      "Epoch [4/30], Step [200/2500], Loss: 1.4885\n",
      "Epoch [4/30], Step [300/2500], Loss: 1.5033\n",
      "Epoch [4/30], Step [400/2500], Loss: 1.4554\n",
      "Epoch [4/30], Step [500/2500], Loss: 1.4454\n",
      "Epoch [4/30], Step [600/2500], Loss: 1.4938\n",
      "Epoch [4/30], Step [700/2500], Loss: 1.4726\n",
      "Epoch [4/30], Step [800/2500], Loss: 1.4806\n",
      "Epoch [4/30], Step [900/2500], Loss: 1.4585\n",
      "Epoch [4/30], Step [1000/2500], Loss: 1.4536\n",
      "Epoch [4/30], Step [1100/2500], Loss: 1.4842\n",
      "Epoch [4/30], Step [1200/2500], Loss: 1.4567\n",
      "Epoch [4/30], Step [1300/2500], Loss: 1.4815\n",
      "Epoch [4/30], Step [1400/2500], Loss: 1.4684\n",
      "Epoch [4/30], Step [1500/2500], Loss: 1.4920\n",
      "Epoch [4/30], Step [1600/2500], Loss: 1.4118\n",
      "Epoch [4/30], Step [1700/2500], Loss: 1.4560\n",
      "Epoch [4/30], Step [1800/2500], Loss: 1.3985\n",
      "Epoch [4/30], Step [1900/2500], Loss: 1.4424\n",
      "Epoch [4/30], Step [2000/2500], Loss: 1.4412\n",
      "Epoch [4/30], Step [2100/2500], Loss: 1.4827\n",
      "Epoch [4/30], Step [2200/2500], Loss: 1.4758\n",
      "Epoch [4/30], Step [2300/2500], Loss: 1.4120\n",
      "Epoch [4/30], Step [2400/2500], Loss: 1.4792\n",
      "Epoch [4/30], Step [2500/2500], Loss: 1.4934\n",
      "Epoch [4/30], Validation Loss: 1.6447, Validation Accuracy: 38.58%\n",
      "Epoch [5/30], Step [100/2500], Loss: 1.4420\n",
      "Epoch [5/30], Step [200/2500], Loss: 1.4343\n",
      "Epoch [5/30], Step [300/2500], Loss: 1.4229\n",
      "Epoch [5/30], Step [400/2500], Loss: 1.5264\n",
      "Epoch [5/30], Step [500/2500], Loss: 1.5127\n",
      "Epoch [5/30], Step [600/2500], Loss: 1.4929\n",
      "Epoch [5/30], Step [700/2500], Loss: 1.4605\n",
      "Epoch [5/30], Step [800/2500], Loss: 1.4205\n",
      "Epoch [5/30], Step [900/2500], Loss: 1.4426\n",
      "Epoch [5/30], Step [1000/2500], Loss: 1.3951\n",
      "Epoch [5/30], Step [1100/2500], Loss: 1.4193\n",
      "Epoch [5/30], Step [1200/2500], Loss: 1.4646\n",
      "Epoch [5/30], Step [1300/2500], Loss: 1.4342\n",
      "Epoch [5/30], Step [1400/2500], Loss: 1.4171\n",
      "Epoch [5/30], Step [1500/2500], Loss: 1.4099\n",
      "Epoch [5/30], Step [1600/2500], Loss: 1.5245\n",
      "Epoch [5/30], Step [1700/2500], Loss: 1.5278\n",
      "Epoch [5/30], Step [1800/2500], Loss: 1.3889\n",
      "Epoch [5/30], Step [1900/2500], Loss: 1.4525\n",
      "Epoch [5/30], Step [2000/2500], Loss: 1.4832\n",
      "Epoch [5/30], Step [2100/2500], Loss: 1.4440\n",
      "Epoch [5/30], Step [2200/2500], Loss: 1.4086\n",
      "Epoch [5/30], Step [2300/2500], Loss: 1.4047\n",
      "Epoch [5/30], Step [2400/2500], Loss: 1.4236\n",
      "Epoch [5/30], Step [2500/2500], Loss: 1.3904\n",
      "Epoch [5/30], Validation Loss: 1.5770, Validation Accuracy: 39.47%\n",
      "Epoch [6/30], Step [100/2500], Loss: 1.4956\n",
      "Epoch [6/30], Step [200/2500], Loss: 1.4356\n",
      "Epoch [6/30], Step [300/2500], Loss: 1.4467\n",
      "Epoch [6/30], Step [400/2500], Loss: 1.4321\n",
      "Epoch [6/30], Step [500/2500], Loss: 1.3737\n",
      "Epoch [6/30], Step [600/2500], Loss: 1.4112\n",
      "Epoch [6/30], Step [700/2500], Loss: 1.3312\n",
      "Epoch [6/30], Step [800/2500], Loss: 1.4313\n",
      "Epoch [6/30], Step [900/2500], Loss: 1.3719\n",
      "Epoch [6/30], Step [1000/2500], Loss: 1.4424\n",
      "Epoch [6/30], Step [1100/2500], Loss: 1.3656\n",
      "Epoch [6/30], Step [1200/2500], Loss: 1.3705\n",
      "Epoch [6/30], Step [1300/2500], Loss: 1.4046\n",
      "Epoch [6/30], Step [1400/2500], Loss: 1.3759\n",
      "Epoch [6/30], Step [1500/2500], Loss: 1.4445\n",
      "Epoch [6/30], Step [1600/2500], Loss: 1.3341\n",
      "Epoch [6/30], Step [1700/2500], Loss: 1.3877\n",
      "Epoch [6/30], Step [1800/2500], Loss: 1.3814\n",
      "Epoch [6/30], Step [1900/2500], Loss: 1.4001\n",
      "Epoch [6/30], Step [2000/2500], Loss: 1.4172\n",
      "Epoch [6/30], Step [2100/2500], Loss: 1.4018\n",
      "Epoch [6/30], Step [2200/2500], Loss: 1.3850\n",
      "Epoch [6/30], Step [2300/2500], Loss: 1.4015\n",
      "Epoch [6/30], Step [2400/2500], Loss: 1.3591\n",
      "Epoch [6/30], Step [2500/2500], Loss: 1.4154\n",
      "Epoch [6/30], Validation Loss: 1.2950, Validation Accuracy: 54.36%\n",
      "Epoch [7/30], Step [100/2500], Loss: 1.3627\n",
      "Epoch [7/30], Step [200/2500], Loss: 1.3598\n",
      "Epoch [7/30], Step [300/2500], Loss: 1.3541\n",
      "Epoch [7/30], Step [400/2500], Loss: 1.4297\n",
      "Epoch [7/30], Step [500/2500], Loss: 1.3791\n",
      "Epoch [7/30], Step [600/2500], Loss: 1.3962\n",
      "Epoch [7/30], Step [700/2500], Loss: 1.4202\n",
      "Epoch [7/30], Step [800/2500], Loss: 1.3271\n",
      "Epoch [7/30], Step [900/2500], Loss: 1.3457\n",
      "Epoch [7/30], Step [1000/2500], Loss: 1.4320\n",
      "Epoch [7/30], Step [1100/2500], Loss: 1.4570\n",
      "Epoch [7/30], Step [1200/2500], Loss: 1.4493\n",
      "Epoch [7/30], Step [1300/2500], Loss: 1.3627\n",
      "Epoch [7/30], Step [1400/2500], Loss: 1.4077\n",
      "Epoch [7/30], Step [1500/2500], Loss: 1.4019\n",
      "Epoch [7/30], Step [1600/2500], Loss: 1.3824\n",
      "Epoch [7/30], Step [1700/2500], Loss: 1.4052\n",
      "Epoch [7/30], Step [1800/2500], Loss: 1.3660\n",
      "Epoch [7/30], Step [1900/2500], Loss: 1.4773\n",
      "Epoch [7/30], Step [2000/2500], Loss: 1.3599\n",
      "Epoch [7/30], Step [2100/2500], Loss: 1.3425\n",
      "Epoch [7/30], Step [2200/2500], Loss: 1.3210\n",
      "Epoch [7/30], Step [2300/2500], Loss: 1.4258\n",
      "Epoch [7/30], Step [2400/2500], Loss: 1.3684\n",
      "Epoch [7/30], Step [2500/2500], Loss: 1.3260\n",
      "Epoch [7/30], Validation Loss: 1.3035, Validation Accuracy: 54.43%\n",
      "Epoch [8/30], Step [100/2500], Loss: 1.3892\n",
      "Epoch [8/30], Step [200/2500], Loss: 1.3896\n",
      "Epoch [8/30], Step [300/2500], Loss: 1.3530\n",
      "Epoch [8/30], Step [400/2500], Loss: 1.3346\n",
      "Epoch [8/30], Step [500/2500], Loss: 1.4333\n",
      "Epoch [8/30], Step [600/2500], Loss: 1.4044\n",
      "Epoch [8/30], Step [700/2500], Loss: 1.3915\n",
      "Epoch [8/30], Step [800/2500], Loss: 1.3703\n",
      "Epoch [8/30], Step [900/2500], Loss: 1.3570\n",
      "Epoch [8/30], Step [1000/2500], Loss: 1.2952\n",
      "Epoch [8/30], Step [1100/2500], Loss: 1.4395\n",
      "Epoch [8/30], Step [1200/2500], Loss: 1.3864\n",
      "Epoch [8/30], Step [1300/2500], Loss: 1.3599\n",
      "Epoch [8/30], Step [1400/2500], Loss: 1.3761\n",
      "Epoch [8/30], Step [1500/2500], Loss: 1.3871\n",
      "Epoch [8/30], Step [1600/2500], Loss: 1.3161\n",
      "Epoch [8/30], Step [1700/2500], Loss: 1.3708\n",
      "Epoch [8/30], Step [1800/2500], Loss: 1.4305\n",
      "Epoch [8/30], Step [1900/2500], Loss: 1.4311\n",
      "Epoch [8/30], Step [2000/2500], Loss: 1.3431\n",
      "Epoch [8/30], Step [2100/2500], Loss: 1.3859\n",
      "Epoch [8/30], Step [2200/2500], Loss: 1.3517\n",
      "Epoch [8/30], Step [2300/2500], Loss: 1.3626\n",
      "Epoch [8/30], Step [2400/2500], Loss: 1.4042\n",
      "Epoch [8/30], Step [2500/2500], Loss: 1.3310\n",
      "Epoch [8/30], Validation Loss: 1.3681, Validation Accuracy: 52.11%\n",
      "Epoch [9/30], Step [100/2500], Loss: 1.3658\n",
      "Epoch [9/30], Step [200/2500], Loss: 1.3795\n",
      "Epoch [9/30], Step [300/2500], Loss: 1.3307\n",
      "Epoch [9/30], Step [400/2500], Loss: 1.3445\n",
      "Epoch [9/30], Step [500/2500], Loss: 1.3852\n",
      "Epoch [9/30], Step [600/2500], Loss: 1.2950\n",
      "Epoch [9/30], Step [700/2500], Loss: 1.2754\n",
      "Epoch [9/30], Step [800/2500], Loss: 1.4411\n",
      "Epoch [9/30], Step [900/2500], Loss: 1.2864\n",
      "Epoch [9/30], Step [1000/2500], Loss: 1.3766\n",
      "Epoch [9/30], Step [1100/2500], Loss: 1.3568\n",
      "Epoch [9/30], Step [1200/2500], Loss: 1.3038\n",
      "Epoch [9/30], Step [1300/2500], Loss: 1.3623\n",
      "Epoch [9/30], Step [1400/2500], Loss: 1.3786\n",
      "Epoch [9/30], Step [1500/2500], Loss: 1.4408\n",
      "Epoch [9/30], Step [1600/2500], Loss: 1.3817\n",
      "Epoch [9/30], Step [1700/2500], Loss: 1.3518\n",
      "Epoch [9/30], Step [1800/2500], Loss: 1.3077\n",
      "Epoch [9/30], Step [1900/2500], Loss: 1.4397\n",
      "Epoch [9/30], Step [2000/2500], Loss: 1.3310\n",
      "Epoch [9/30], Step [2100/2500], Loss: 1.3624\n",
      "Epoch [9/30], Step [2200/2500], Loss: 1.4422\n",
      "Epoch [9/30], Step [2300/2500], Loss: 1.3684\n",
      "Epoch [9/30], Step [2400/2500], Loss: 1.3557\n",
      "Epoch [9/30], Step [2500/2500], Loss: 1.3575\n",
      "Epoch [9/30], Validation Loss: 1.5690, Validation Accuracy: 45.94%\n",
      "Epoch [10/30], Step [100/2500], Loss: 1.3917\n",
      "Epoch [10/30], Step [200/2500], Loss: 1.3519\n",
      "Epoch [10/30], Step [300/2500], Loss: 1.3778\n",
      "Epoch [10/30], Step [400/2500], Loss: 1.3371\n",
      "Epoch [10/30], Step [500/2500], Loss: 1.3856\n",
      "Epoch [10/30], Step [600/2500], Loss: 1.3088\n",
      "Epoch [10/30], Step [700/2500], Loss: 1.4286\n",
      "Epoch [10/30], Step [800/2500], Loss: 1.4223\n",
      "Epoch [10/30], Step [900/2500], Loss: 1.3560\n",
      "Epoch [10/30], Step [1000/2500], Loss: 1.4043\n",
      "Epoch [10/30], Step [1100/2500], Loss: 1.3471\n",
      "Epoch [10/30], Step [1200/2500], Loss: 1.3390\n",
      "Epoch [10/30], Step [1300/2500], Loss: 1.3772\n",
      "Epoch [10/30], Step [1400/2500], Loss: 1.3320\n",
      "Epoch [10/30], Step [1500/2500], Loss: 1.3885\n",
      "Epoch [10/30], Step [1600/2500], Loss: 1.3289\n",
      "Epoch [10/30], Step [1700/2500], Loss: 1.3649\n",
      "Epoch [10/30], Step [1800/2500], Loss: 1.3402\n",
      "Epoch [10/30], Step [1900/2500], Loss: 1.3498\n",
      "Epoch [10/30], Step [2000/2500], Loss: 1.4101\n",
      "Epoch [10/30], Step [2100/2500], Loss: 1.5099\n",
      "Epoch [10/30], Step [2200/2500], Loss: 1.3365\n",
      "Epoch [10/30], Step [2300/2500], Loss: 1.4282\n",
      "Epoch [10/30], Step [2400/2500], Loss: 1.2751\n",
      "Epoch [10/30], Step [2500/2500], Loss: 1.3409\n",
      "Epoch [10/30], Validation Loss: 1.4864, Validation Accuracy: 50.08%\n",
      "Epoch [11/30], Step [100/2500], Loss: 1.3638\n",
      "Epoch [11/30], Step [200/2500], Loss: 1.3273\n",
      "Epoch [11/30], Step [300/2500], Loss: 1.4391\n",
      "Epoch [11/30], Step [400/2500], Loss: 1.3723\n",
      "Epoch [11/30], Step [500/2500], Loss: 1.2659\n",
      "Epoch [11/30], Step [600/2500], Loss: 1.4008\n",
      "Epoch [11/30], Step [700/2500], Loss: 1.3454\n",
      "Epoch [11/30], Step [800/2500], Loss: 1.3492\n",
      "Epoch [11/30], Step [900/2500], Loss: 1.4130\n",
      "Epoch [11/30], Step [1000/2500], Loss: 1.4342\n",
      "Epoch [11/30], Step [1100/2500], Loss: 1.3447\n",
      "Epoch [11/30], Step [1200/2500], Loss: 1.4613\n",
      "Epoch [11/30], Step [1300/2500], Loss: 1.3804\n",
      "Epoch [11/30], Step [1400/2500], Loss: 1.3887\n",
      "Epoch [11/30], Step [1500/2500], Loss: 1.2898\n",
      "Epoch [11/30], Step [1600/2500], Loss: 1.4038\n",
      "Epoch [11/30], Step [1700/2500], Loss: 1.4135\n",
      "Epoch [11/30], Step [1800/2500], Loss: 1.3562\n",
      "Epoch [11/30], Step [1900/2500], Loss: 1.4186\n",
      "Epoch [11/30], Step [2000/2500], Loss: 1.3246\n",
      "Epoch [11/30], Step [2100/2500], Loss: 1.3287\n",
      "Epoch [11/30], Step [2200/2500], Loss: 1.4152\n",
      "Epoch [11/30], Step [2300/2500], Loss: 1.2946\n",
      "Epoch [11/30], Step [2400/2500], Loss: 1.3945\n",
      "Epoch [11/30], Step [2500/2500], Loss: 1.3959\n",
      "Epoch [11/30], Validation Loss: 1.3537, Validation Accuracy: 48.68%\n",
      "Epoch [12/30], Step [100/2500], Loss: 1.3358\n",
      "Epoch [12/30], Step [200/2500], Loss: 1.3228\n",
      "Epoch [12/30], Step [300/2500], Loss: 1.3611\n",
      "Epoch [12/30], Step [400/2500], Loss: 1.4218\n",
      "Epoch [12/30], Step [500/2500], Loss: 1.3777\n",
      "Epoch [12/30], Step [600/2500], Loss: 1.2682\n",
      "Epoch [12/30], Step [700/2500], Loss: 1.4200\n",
      "Epoch [12/30], Step [800/2500], Loss: 1.5177\n",
      "Epoch [12/30], Step [900/2500], Loss: 1.4368\n",
      "Epoch [12/30], Step [1000/2500], Loss: 1.4026\n",
      "Epoch [12/30], Step [1100/2500], Loss: 1.4259\n",
      "Epoch [12/30], Step [1200/2500], Loss: 1.4526\n",
      "Epoch [12/30], Step [1300/2500], Loss: 1.3637\n",
      "Epoch [12/30], Step [1400/2500], Loss: 1.3397\n",
      "Epoch [12/30], Step [1500/2500], Loss: 1.3550\n",
      "Epoch [12/30], Step [1600/2500], Loss: 1.3510\n",
      "Epoch [12/30], Step [1700/2500], Loss: 1.2557\n",
      "Epoch [12/30], Step [1800/2500], Loss: 1.3107\n",
      "Epoch [12/30], Step [1900/2500], Loss: 1.2344\n",
      "Epoch [12/30], Step [2000/2500], Loss: 1.3567\n",
      "Epoch [12/30], Step [2100/2500], Loss: 1.3719\n",
      "Epoch [12/30], Step [2200/2500], Loss: 1.4129\n",
      "Epoch [12/30], Step [2300/2500], Loss: 1.3454\n",
      "Epoch [12/30], Step [2400/2500], Loss: 1.3106\n",
      "Epoch [12/30], Step [2500/2500], Loss: 1.4183\n",
      "Epoch [12/30], Validation Loss: 1.4174, Validation Accuracy: 51.62%\n",
      "Epoch [13/30], Step [100/2500], Loss: 1.3305\n",
      "Epoch [13/30], Step [200/2500], Loss: 1.3607\n",
      "Epoch [13/30], Step [300/2500], Loss: 1.3095\n",
      "Epoch [13/30], Step [400/2500], Loss: 1.3877\n",
      "Epoch [13/30], Step [500/2500], Loss: 1.3117\n",
      "Epoch [13/30], Step [600/2500], Loss: 1.4351\n",
      "Epoch [13/30], Step [700/2500], Loss: 1.3276\n",
      "Epoch [13/30], Step [800/2500], Loss: 1.3114\n",
      "Epoch [13/30], Step [900/2500], Loss: 1.3381\n",
      "Epoch [13/30], Step [1000/2500], Loss: 1.3810\n",
      "Epoch [13/30], Step [1100/2500], Loss: 1.3975\n",
      "Epoch [13/30], Step [1200/2500], Loss: 1.3357\n",
      "Epoch [13/30], Step [1300/2500], Loss: 1.3217\n",
      "Epoch [13/30], Step [1400/2500], Loss: 1.3696\n",
      "Epoch [13/30], Step [1500/2500], Loss: 1.3332\n",
      "Epoch [13/30], Step [1600/2500], Loss: 1.3420\n",
      "Epoch [13/30], Step [1700/2500], Loss: 1.2869\n",
      "Epoch [13/30], Step [1800/2500], Loss: 1.3048\n",
      "Epoch [13/30], Step [1900/2500], Loss: 1.3281\n",
      "Epoch [13/30], Step [2000/2500], Loss: 1.3271\n",
      "Epoch [13/30], Step [2100/2500], Loss: 1.2557\n",
      "Epoch [13/30], Step [2200/2500], Loss: 1.2978\n",
      "Epoch [13/30], Step [2300/2500], Loss: 1.2929\n",
      "Epoch [13/30], Step [2400/2500], Loss: 1.2993\n",
      "Epoch [13/30], Step [2500/2500], Loss: 1.4162\n",
      "Epoch [13/30], Validation Loss: 1.4900, Validation Accuracy: 46.54%\n",
      "Epoch [14/30], Step [100/2500], Loss: 1.3867\n",
      "Epoch [14/30], Step [200/2500], Loss: 1.3977\n",
      "Epoch [14/30], Step [300/2500], Loss: 1.2680\n",
      "Epoch [14/30], Step [400/2500], Loss: 1.3272\n",
      "Epoch [14/30], Step [500/2500], Loss: 1.3482\n",
      "Epoch [14/30], Step [600/2500], Loss: 1.3593\n",
      "Epoch [14/30], Step [700/2500], Loss: 1.3605\n",
      "Epoch [14/30], Step [800/2500], Loss: 1.2576\n",
      "Epoch [14/30], Step [900/2500], Loss: 1.2999\n",
      "Epoch [14/30], Step [1000/2500], Loss: 1.2980\n",
      "Epoch [14/30], Step [1100/2500], Loss: 1.2989\n",
      "Epoch [14/30], Step [1200/2500], Loss: 1.3004\n",
      "Epoch [14/30], Step [1300/2500], Loss: 1.3276\n",
      "Epoch [14/30], Step [1400/2500], Loss: 1.3113\n",
      "Epoch [14/30], Step [1500/2500], Loss: 1.4050\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/30], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch + 1}/30], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "torch.save(model.state_dict(), \"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 76.99 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pruned VGG19 (TODO!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_probs, teacher_probs, labels, alpha):\n",
    "    kl_div_loss = nn.functional.kl_div(student_probs, teacher_probs, reduction=\"batchmean\")\n",
    "    ce_loss = nn.functional.cross_entropy(student_probs, labels)\n",
    "    return alpha * kl_div_loss + (1 - alpha) * ce_loss\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
